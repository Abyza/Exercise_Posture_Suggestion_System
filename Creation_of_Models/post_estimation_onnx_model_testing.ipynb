{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Models Speed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Processor:     \t12th Gen Intel(R) Core(TM) i5-12500H, 3100 Mhz, 12 Core(s), 16 Logical Processor(s)\n",
    "\n",
    "System Model:\tNitro AN515-58"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: onnx in d:\\datasci\\finalproject\\onnx_model_compilation\\.venv\\lib\\site-packages (1.16.0)\n",
      "Collecting onnxruntime\n",
      "  Downloading onnxruntime-1.18.0-cp312-cp312-win_amd64.whl.metadata (4.4 kB)\n",
      "Requirement already satisfied: numpy>=1.20 in d:\\datasci\\finalproject\\onnx_model_compilation\\.venv\\lib\\site-packages (from onnx) (1.26.4)\n",
      "Requirement already satisfied: protobuf>=3.20.2 in d:\\datasci\\finalproject\\onnx_model_compilation\\.venv\\lib\\site-packages (from onnx) (5.26.1)\n",
      "Collecting coloredlogs (from onnxruntime)\n",
      "  Using cached coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
      "Collecting flatbuffers (from onnxruntime)\n",
      "  Using cached flatbuffers-24.3.25-py2.py3-none-any.whl.metadata (850 bytes)\n",
      "Requirement already satisfied: packaging in d:\\datasci\\finalproject\\onnx_model_compilation\\.venv\\lib\\site-packages (from onnxruntime) (24.0)\n",
      "Collecting sympy (from onnxruntime)\n",
      "  Using cached sympy-1.12-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime)\n",
      "  Using cached humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
      "Collecting mpmath>=0.19 (from sympy->onnxruntime)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting pyreadline3 (from humanfriendly>=9.1->coloredlogs->onnxruntime)\n",
      "  Using cached pyreadline3-3.4.1-py3-none-any.whl.metadata (2.0 kB)\n",
      "Downloading onnxruntime-1.18.0-cp312-cp312-win_amd64.whl (5.6 MB)\n",
      "   ---------------------------------------- 0.0/5.6 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.1/5.6 MB 3.2 MB/s eta 0:00:02\n",
      "   --- ------------------------------------ 0.5/5.6 MB 5.8 MB/s eta 0:00:01\n",
      "   ------- -------------------------------- 1.1/5.6 MB 8.4 MB/s eta 0:00:01\n",
      "   ----------- ---------------------------- 1.6/5.6 MB 10.0 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 2.2/5.6 MB 10.8 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 3.0/5.6 MB 11.1 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 3.9/5.6 MB 12.4 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 5.1/5.6 MB 14.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 5.6/5.6 MB 14.9 MB/s eta 0:00:00\n",
      "Using cached coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
      "Using cached flatbuffers-24.3.25-py2.py3-none-any.whl (26 kB)\n",
      "Using cached sympy-1.12-py3-none-any.whl (5.7 MB)\n",
      "Using cached humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Using cached pyreadline3-3.4.1-py3-none-any.whl (95 kB)\n",
      "Installing collected packages: pyreadline3, mpmath, flatbuffers, sympy, humanfriendly, coloredlogs, onnxruntime\n",
      "Successfully installed coloredlogs-15.0.1 flatbuffers-24.3.25 humanfriendly-10.0 mpmath-1.3.0 onnxruntime-1.18.0 pyreadline3-3.4.1 sympy-1.12\n"
     ]
    }
   ],
   "source": [
    "!pip install onnx onnxruntime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 0.04138946533203125 seconds\n"
     ]
    }
   ],
   "source": [
    "import onnx\n",
    "import onnxruntime as ort\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "def inference_onnx_model(model_path, input_shape):\n",
    "    # Load the ONNX model\n",
    "    onnx_model = onnx.load(model_path)\n",
    "\n",
    "    # Create an ONNX runtime inference session\n",
    "    session = ort.InferenceSession(model_path)\n",
    "\n",
    "    # Get the input name of the model\n",
    "    input_name = session.get_inputs()[0].name\n",
    "\n",
    "    # Generate sample input data\n",
    "    input_data = np.random.rand(*input_shape).astype(np.float32)\n",
    "\n",
    "    # Perform inference and measure execution time\n",
    "    start_time = time.time()\n",
    "    output = session.run(None, {input_name: input_data})\n",
    "    end_time = time.time()\n",
    "\n",
    "    # Calculate execution time\n",
    "    execution_time = end_time - start_time\n",
    "\n",
    "    return output, execution_time\n",
    "\n",
    "# Example usage:\n",
    "model_path = 'model_1_post_estimation_resnet.onnx'\n",
    "input_shape = (1, 3, 256, 256)\n",
    "\n",
    "output, execution_time = inference_onnx_model(model_path, input_shape)\n",
    "print(\"Execution time:\", execution_time, \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_average_execution_time(model_path, input_shape, num_tests=100):\n",
    "    \"\"\"\n",
    "    Measure the average execution time of inference on an ONNX model.\n",
    "\n",
    "    Args:\n",
    "    - model_path (str): Path to the ONNX model file.\n",
    "    - input_shape (tuple): Shape of the input data for inference.\n",
    "    - num_tests (int, optional): Number of times to run the inference (default is 100).\n",
    "\n",
    "    Returns:\n",
    "    - float: Average execution time in seconds.\n",
    "    \"\"\"\n",
    "    # Initialize an empty list to store execution times\n",
    "    execution_times = []\n",
    "\n",
    "    # Run the function multiple times\n",
    "    for _ in range(num_tests):\n",
    "        output, execution_time = inference_onnx_model(model_path, input_shape)\n",
    "        execution_times.append(execution_time)\n",
    "    \n",
    "    print(len(execution_times))\n",
    "\n",
    "    # Calculate the average execution time\n",
    "    average_execution_time = np.mean(execution_times)\n",
    "\n",
    "    return average_execution_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "100\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "model_path = 'model_1_post_estimation_resnet.onnx'\n",
    "input_shape = (1, 3, 256, 256)\n",
    "\n",
    "model_1_post_estimation_resnet_time = measure_average_execution_time(model_path, input_shape, num_tests=100)\n",
    "\n",
    "model_path = 'model_2_post_estimation_yolo_v8.onnx'\n",
    "input_shape = (1, 3, 640, 640)\n",
    "\n",
    "model_2_post_estimation_yolo_v8_time = measure_average_execution_time(model_path, input_shape, num_tests=100)\n",
    "\n",
    "model_path = 'model_3_post_estimation_yolo_nas.onnx'\n",
    "input_shape = (1, 3, 640, 640)\n",
    "\n",
    "model_3_post_estimation_yolo_nas_time = measure_average_execution_time(model_path, input_shape, num_tests=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Execution Time 100 samples\n",
      "ResNet: 0.033821325302124026\n",
      "YOLOv8: 0.0809701919555664\n",
      "YOLO-NAS: 0.08244905948638916\n"
     ]
    }
   ],
   "source": [
    "print('Average Execution Time 100 samples')\n",
    "print('ResNet:', model_1_post_estimation_resnet_time)\n",
    "print('YOLOv8:', model_2_post_estimation_yolo_v8_time)\n",
    "print('YOLO-NAS:', model_3_post_estimation_yolo_nas_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "100\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "model_path = 'model_1_post_estimation_resnet.onnx'\n",
    "input_shape = (1, 3, 256, 256)\n",
    "\n",
    "model_1_post_estimation_resnet_time = measure_average_execution_time(model_path, input_shape, num_tests=100)\n",
    "\n",
    "model_path = 'model_2_post_estimation_yolo_v8.onnx'\n",
    "input_shape = (1, 3, 640, 640)\n",
    "\n",
    "model_2_post_estimation_yolo_v8_time = measure_average_execution_time(model_path, input_shape, num_tests=100)\n",
    "\n",
    "model_path = 'model_3_post_estimation_yolo_nas.onnx'\n",
    "input_shape = (1, 3, 640, 640)\n",
    "\n",
    "model_3_post_estimation_yolo_nas_time = measure_average_execution_time(model_path, input_shape, num_tests=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Execution Time 100 samples\n",
      "ResNet: 0.033288047313690186\n",
      "YOLOv8: 0.07952313423156739\n",
      "YOLO-NAS: 0.08013875246047973\n"
     ]
    }
   ],
   "source": [
    "print('Average Execution Time 100 samples')\n",
    "print('ResNet:', model_1_post_estimation_resnet_time)\n",
    "print('YOLOv8:', model_2_post_estimation_yolo_v8_time)\n",
    "print('YOLO-NAS:', model_3_post_estimation_yolo_nas_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Model                       | ResNet_256_50    |YOLOv8_s          |YOLO-NAS_s        |\n",
    "|-----------------------------|------------------|------------------|------------------|\n",
    "| mAP (Mean Average Precision)| 88.5             | 86.2             | 88.8             |\n",
    "| Time on Laptop CPU          | 0.0333 sec       | 0.0795 sec       | 0.0801 sec       |\n",
    "| Time on Google Collab CPU   | 0.2658 sec       | 0.6005 sec       | 0.7228 sec       |\n",
    "| Number of layers            | 129              | 311              | 321              |\n",
    "| Number of weights           | 123              | 145              | 263              |\n",
    "| Onnx Model Size             | 132.7 mb         | 45.7 mb          | 60.2 mb          |\n",
    "| Input shape                 | (1, 3, 256, 256) | (1, 3, 640, 640) | (1, 3, 640, 640) |\n",
    "| Output shape                | (1, 16, 64, 64)  | (1, 56, 8400)    | (1, 8400, 4)     |\n",
    "| # of Keypoints              | 16               | 17               | 17               |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
